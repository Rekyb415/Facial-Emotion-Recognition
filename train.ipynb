{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d185233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-macosx_10_15_x86_64.whl (230.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.1/230.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.11.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.22 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (65.6.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-macosx_10_9_x86_64.whl (26.7 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: packaging in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2.tar.gz (23.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.2-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.10.0)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-macosx_10_9_universal2.whl (317 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.9/317.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.14)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/BlueItMy03/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: grpcio, jax\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpcio: filename=grpcio-1.54.2-cp310-cp310-macosx_10_10_x86_64.whl size=4095991 sha256=9733b4dc9907437095e69732cb03ebceba882cac9a7a74fe301d076227a2328a\n",
      "  Stored in directory: /Users/BlueItMy03/Library/Caches/pip/wheels/cb/30/86/ed846d9ef9447372ba80a74992919a1257d827024f3324c535\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.11-py3-none-any.whl size=1487877 sha256=e1b69bb9004825d73f44009ab3e5b3e1435263c1f8753098c3928d11e7ef2b4c\n",
      "  Stored in directory: /Users/BlueItMy03/Library/Caches/pip/wheels/60/ba/dd/b66da6df64fba82ddb4401361647a80bb117e33b1fbca4d0b5\n",
      "Successfully built grpcio jax\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 jax-0.4.11 keras-2.12.0 libclang-16.0.0 ml-dtypes-0.1.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c16b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 22:52:42.796240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "happiness\n",
      "surprise\n",
      "sadness\n",
      "anger\n",
      "disgust\n",
      "fear\n",
      "contempt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "### loading data\n",
    "labels = {'neutral': 0,\n",
    "          'happiness': 1,\n",
    "          'surprise': 2,\n",
    "          'sadness': 3,\n",
    "          'anger': 4,\n",
    "          'disgust': 5,\n",
    "          'fear': 6,\n",
    "          'contempt': 7}\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    for filename in os.listdir(\"FER+/emotion_data/\" + label):\n",
    "        if (\n",
    "            filename.split(\".\").pop().lower() == \"png\"\n",
    "        ):\n",
    "            img = load_img(\n",
    "                \"FER+/emotion_data/\" + label + \"/\" + filename,\n",
    "                target_size=(48, 48),\n",
    "                color_mode=\"grayscale\",\n",
    "            )\n",
    "            img = img_to_array(img)\n",
    "            img = img / 255.0\n",
    "            x.append(img)\n",
    "            y.append(labels[label])\n",
    "\n",
    "x = np.array(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df926ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.50980395],\n",
       "         [0.42745098],\n",
       "         [0.3882353 ],\n",
       "         ...,\n",
       "         [0.39215687],\n",
       "         [0.44705883],\n",
       "         [0.56078434]],\n",
       "\n",
       "        [[0.5176471 ],\n",
       "         [0.4745098 ],\n",
       "         [0.45882353],\n",
       "         ...,\n",
       "         [0.41960785],\n",
       "         [0.34901962],\n",
       "         [0.36862746]],\n",
       "\n",
       "        [[0.5647059 ],\n",
       "         [0.49019608],\n",
       "         [0.49411765],\n",
       "         ...,\n",
       "         [0.52156866],\n",
       "         [0.41568628],\n",
       "         [0.34901962]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         ...,\n",
       "         [0.78431374],\n",
       "         [0.8039216 ],\n",
       "         [0.8117647 ]],\n",
       "\n",
       "        [[0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         ...,\n",
       "         [0.7921569 ],\n",
       "         [0.8039216 ],\n",
       "         [0.79607844]],\n",
       "\n",
       "        [[0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         [0.6039216 ],\n",
       "         ...,\n",
       "         [0.78431374],\n",
       "         [0.79607844],\n",
       "         [0.7921569 ]]],\n",
       "\n",
       "\n",
       "       [[[0.5529412 ],\n",
       "         [0.6392157 ],\n",
       "         [0.67058825],\n",
       "         ...,\n",
       "         [0.7058824 ],\n",
       "         [0.7019608 ],\n",
       "         [0.69803923]],\n",
       "\n",
       "        [[0.6117647 ],\n",
       "         [0.65882355],\n",
       "         [0.7019608 ],\n",
       "         ...,\n",
       "         [0.73333335],\n",
       "         [0.7254902 ],\n",
       "         [0.7294118 ]],\n",
       "\n",
       "        [[0.58431375],\n",
       "         [0.64705884],\n",
       "         [0.70980394],\n",
       "         ...,\n",
       "         [0.76862746],\n",
       "         [0.75686276],\n",
       "         [0.7647059 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.7882353 ],\n",
       "         [0.84705883],\n",
       "         [0.85490197],\n",
       "         ...,\n",
       "         [0.93333334],\n",
       "         [0.9254902 ],\n",
       "         [0.9254902 ]],\n",
       "\n",
       "        [[0.8       ],\n",
       "         [0.827451  ],\n",
       "         [0.8392157 ],\n",
       "         ...,\n",
       "         [0.92941177],\n",
       "         [0.9254902 ],\n",
       "         [0.92941177]],\n",
       "\n",
       "        [[0.8156863 ],\n",
       "         [0.8235294 ],\n",
       "         [0.8117647 ],\n",
       "         ...,\n",
       "         [0.92941177],\n",
       "         [0.9254902 ],\n",
       "         [0.92941177]]],\n",
       "\n",
       "\n",
       "       [[[0.79607844],\n",
       "         [0.42745098],\n",
       "         [0.44705883],\n",
       "         ...,\n",
       "         [1.        ],\n",
       "         [1.        ],\n",
       "         [1.        ]],\n",
       "\n",
       "        [[0.41568628],\n",
       "         [0.42352942],\n",
       "         [0.59607846],\n",
       "         ...,\n",
       "         [1.        ],\n",
       "         [1.        ],\n",
       "         [1.        ]],\n",
       "\n",
       "        [[0.40784314],\n",
       "         [0.59607846],\n",
       "         [0.6156863 ],\n",
       "         ...,\n",
       "         [1.        ],\n",
       "         [1.        ],\n",
       "         [1.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.43137255],\n",
       "         [0.34117648],\n",
       "         [0.3254902 ],\n",
       "         ...,\n",
       "         [0.30588236],\n",
       "         [0.3254902 ],\n",
       "         [0.34509805]],\n",
       "\n",
       "        [[0.4627451 ],\n",
       "         [0.36862746],\n",
       "         [0.36078432],\n",
       "         ...,\n",
       "         [0.39607844],\n",
       "         [0.3372549 ],\n",
       "         [0.35686275]],\n",
       "\n",
       "        [[0.4392157 ],\n",
       "         [0.39607844],\n",
       "         [0.4117647 ],\n",
       "         ...,\n",
       "         [0.45882353],\n",
       "         [0.38039216],\n",
       "         [0.34901962]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.03529412],\n",
       "         [0.03921569],\n",
       "         [0.10196079],\n",
       "         ...,\n",
       "         [0.2784314 ],\n",
       "         [0.4392157 ],\n",
       "         [0.73333335]],\n",
       "\n",
       "        [[0.03921569],\n",
       "         [0.03137255],\n",
       "         [0.11764706],\n",
       "         ...,\n",
       "         [0.27058825],\n",
       "         [0.41568628],\n",
       "         [0.70980394]],\n",
       "\n",
       "        [[0.03529412],\n",
       "         [0.02745098],\n",
       "         [0.04705882],\n",
       "         ...,\n",
       "         [0.26666668],\n",
       "         [0.39215687],\n",
       "         [0.68235296]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.09411765],\n",
       "         [0.09803922],\n",
       "         [0.23921569],\n",
       "         ...,\n",
       "         [0.11764706],\n",
       "         [0.12156863],\n",
       "         [0.1254902 ]],\n",
       "\n",
       "        [[0.09411765],\n",
       "         [0.10196079],\n",
       "         [0.20392157],\n",
       "         ...,\n",
       "         [0.12156863],\n",
       "         [0.12941177],\n",
       "         [0.1254902 ]],\n",
       "\n",
       "        [[0.10196079],\n",
       "         [0.11764706],\n",
       "         [0.16470589],\n",
       "         ...,\n",
       "         [0.11764706],\n",
       "         [0.12941177],\n",
       "         [0.12941177]]],\n",
       "\n",
       "\n",
       "       [[[0.17254902],\n",
       "         [0.22352941],\n",
       "         [0.34901962],\n",
       "         ...,\n",
       "         [0.12156863],\n",
       "         [0.14117648],\n",
       "         [0.14901961]],\n",
       "\n",
       "        [[0.10588235],\n",
       "         [0.1764706 ],\n",
       "         [0.3254902 ],\n",
       "         ...,\n",
       "         [0.16862746],\n",
       "         [0.10588235],\n",
       "         [0.14901961]],\n",
       "\n",
       "        [[0.10588235],\n",
       "         [0.1882353 ],\n",
       "         [0.27450982],\n",
       "         ...,\n",
       "         [0.3254902 ],\n",
       "         [0.07450981],\n",
       "         [0.1254902 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8039216 ],\n",
       "         [0.8039216 ],\n",
       "         [0.8       ],\n",
       "         ...,\n",
       "         [0.6627451 ],\n",
       "         [0.6666667 ],\n",
       "         [0.6627451 ]],\n",
       "\n",
       "        [[0.8       ],\n",
       "         [0.79607844],\n",
       "         [0.7921569 ],\n",
       "         ...,\n",
       "         [0.6627451 ],\n",
       "         [0.65882355],\n",
       "         [0.654902  ]],\n",
       "\n",
       "        [[0.79607844],\n",
       "         [0.7921569 ],\n",
       "         [0.7882353 ],\n",
       "         ...,\n",
       "         [0.65882355],\n",
       "         [0.6509804 ],\n",
       "         [0.6509804 ]]],\n",
       "\n",
       "\n",
       "       [[[0.21960784],\n",
       "         [0.21960784],\n",
       "         [0.21176471],\n",
       "         ...,\n",
       "         [0.0627451 ],\n",
       "         [0.07058824],\n",
       "         [0.0627451 ]],\n",
       "\n",
       "        [[0.22352941],\n",
       "         [0.22352941],\n",
       "         [0.21568628],\n",
       "         ...,\n",
       "         [0.07058824],\n",
       "         [0.07450981],\n",
       "         [0.06666667]],\n",
       "\n",
       "        [[0.22352941],\n",
       "         [0.22352941],\n",
       "         [0.21568628],\n",
       "         ...,\n",
       "         [0.08627451],\n",
       "         [0.07450981],\n",
       "         [0.07843138]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.02352941],\n",
       "         [0.04313726],\n",
       "         [0.04705882],\n",
       "         ...,\n",
       "         [0.2784314 ],\n",
       "         [0.29803923],\n",
       "         [0.10196079]],\n",
       "\n",
       "        [[0.03529412],\n",
       "         [0.04705882],\n",
       "         [0.03921569],\n",
       "         ...,\n",
       "         [0.28235295],\n",
       "         [0.29803923],\n",
       "         [0.10196079]],\n",
       "\n",
       "        [[0.03137255],\n",
       "         [0.01960784],\n",
       "         [0.01176471],\n",
       "         ...,\n",
       "         [0.30588236],\n",
       "         [0.29411766],\n",
       "         [0.07450981]]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78ddca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle= True)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b70b0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 512)       590336    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 12, 12, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,479,240\n",
      "Trainable params: 4,475,272\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BlueItMy03/anaconda3/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "\n",
    "no_of_classes = 8\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#1st CNN layer\n",
    "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#2nd CNN layer\n",
    "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))\n",
    "\n",
    "#3rd CNN layer\n",
    "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout (0.25))#4th CNN layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Fully connected 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(no_of_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = Adam(lr = 0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766dfa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"run app/model.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c69a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7519 - accuracy: 0.3644\n",
      "Epoch 1: accuracy improved from -inf to 0.36435, saving model to run app/model.h5\n",
      "500/500 [==============================] - 504s 1s/step - loss: 1.7519 - accuracy: 0.3644 - lr: 1.0000e-04\n",
      "Epoch 2/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.4613 - accuracy: 0.4820\n",
      "Epoch 2: accuracy improved from 0.36435 to 0.48204, saving model to run app/model.h5\n",
      "500/500 [==============================] - 516s 1s/step - loss: 1.4613 - accuracy: 0.4820 - lr: 1.0000e-04\n",
      "Epoch 3/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.3195 - accuracy: 0.5373\n",
      "Epoch 3: accuracy improved from 0.48204 to 0.53727, saving model to run app/model.h5\n",
      "500/500 [==============================] - 494s 989ms/step - loss: 1.3195 - accuracy: 0.5373 - lr: 1.0000e-04\n",
      "Epoch 4/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.2200 - accuracy: 0.5724\n",
      "Epoch 4: accuracy improved from 0.53727 to 0.57243, saving model to run app/model.h5\n",
      "500/500 [==============================] - 589s 1s/step - loss: 1.2200 - accuracy: 0.5724 - lr: 1.0000e-04\n",
      "Epoch 5/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.1191 - accuracy: 0.6057\n",
      "Epoch 5: accuracy improved from 0.57243 to 0.60572, saving model to run app/model.h5\n",
      "500/500 [==============================] - 562s 1s/step - loss: 1.1191 - accuracy: 0.6057 - lr: 1.0000e-04\n",
      "Epoch 6/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.0583 - accuracy: 0.6264\n",
      "Epoch 6: accuracy improved from 0.60572 to 0.62641, saving model to run app/model.h5\n",
      "500/500 [==============================] - 582s 1s/step - loss: 1.0583 - accuracy: 0.6264 - lr: 1.0000e-04\n",
      "Epoch 7/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.9983 - accuracy: 0.6416\n",
      "Epoch 7: accuracy improved from 0.62641 to 0.64160, saving model to run app/model.h5\n",
      "500/500 [==============================] - 679s 1s/step - loss: 0.9983 - accuracy: 0.6416 - lr: 1.0000e-04\n",
      "Epoch 8/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.9459 - accuracy: 0.6640\n",
      "Epoch 8: accuracy improved from 0.64160 to 0.66398, saving model to run app/model.h5\n",
      "500/500 [==============================] - 654s 1s/step - loss: 0.9459 - accuracy: 0.6640 - lr: 1.0000e-04\n",
      "Epoch 9/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6744\n",
      "Epoch 9: accuracy improved from 0.66398 to 0.67441, saving model to run app/model.h5\n",
      "500/500 [==============================] - 624s 1s/step - loss: 0.9034 - accuracy: 0.6744 - lr: 1.0000e-04\n",
      "Epoch 10/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.8732 - accuracy: 0.6847\n",
      "Epoch 10: accuracy improved from 0.67441 to 0.68468, saving model to run app/model.h5\n",
      "500/500 [==============================] - 673s 1s/step - loss: 0.8732 - accuracy: 0.6847 - lr: 1.0000e-04\n",
      "Epoch 11/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.8367 - accuracy: 0.6972\n",
      "Epoch 11: accuracy improved from 0.68468 to 0.69724, saving model to run app/model.h5\n",
      "500/500 [==============================] - 737s 1s/step - loss: 0.8367 - accuracy: 0.6972 - lr: 1.0000e-04\n",
      "Epoch 12/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.7095\n",
      "Epoch 12: accuracy improved from 0.69724 to 0.70951, saving model to run app/model.h5\n",
      "500/500 [==============================] - 678s 1s/step - loss: 0.8042 - accuracy: 0.7095 - lr: 1.0000e-04\n",
      "Epoch 13/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.7772 - accuracy: 0.7181\n",
      "Epoch 13: accuracy improved from 0.70951 to 0.71809, saving model to run app/model.h5\n",
      "500/500 [==============================] - 654s 1s/step - loss: 0.7772 - accuracy: 0.7181 - lr: 1.0000e-04\n",
      "Epoch 14/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.7524 - accuracy: 0.7295\n",
      "Epoch 14: accuracy improved from 0.71809 to 0.72948, saving model to run app/model.h5\n",
      "500/500 [==============================] - 615s 1s/step - loss: 0.7524 - accuracy: 0.7295 - lr: 1.0000e-04\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.7239 - accuracy: 0.7418\n",
      "Epoch 15: accuracy improved from 0.72948 to 0.74179, saving model to run app/model.h5\n",
      "500/500 [==============================] - 594s 1s/step - loss: 0.7239 - accuracy: 0.7418 - lr: 1.0000e-04\n",
      "111/111 [==============================] - 19s 164ms/step - loss: 0.6788 - accuracy: 0.7563\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=15, batch_size=64, callbacks=callbacks_list)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "##print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fa45c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer : Adam\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper right\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305016f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
